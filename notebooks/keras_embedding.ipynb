{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of text_sentiment_ngrams_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qkt0rqbRxG7",
        "colab_type": "text"
      },
      "source": [
        "**Read stuff from your own gdrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M0RQ8BR6k2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MY_HOME = 'drive/My Drive/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdbZAy0W9P3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q-lOOjJ5kH0",
        "colab_type": "code",
        "outputId": "1f430ac6-feef-4550-d04f-59ad6e5c3581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "\n",
        "def to_tdidf(pickle_path=\"tdidf_method.pkl\"):\n",
        "    print(\"==========> Data Preprocessing\")\n",
        "    data = pd.read_pickle(os.path.join(MY_HOME, \"preprocessed_training_dataframe.pkl\"))\n",
        "\n",
        "    print(data.head())\n",
        "    lim_unigram = 5000\n",
        "    # no more ram, limit this\n",
        "    vectorizer = TfidfVectorizer(max_features=lim_unigram)\n",
        "    vectorizer.fit(data['X'][0:15555])\n",
        "\n",
        "    # fp = os.path.join(MY_HOME, pickle_path)\n",
        "    # with open(fp, 'wb') as jar:\n",
        "    #     pickle.dump(vectorizer, jar, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    return vectorizer\n",
        "\n",
        "def fc(vectorize, denselist, data):\n",
        "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50), random_state=1)\n",
        "    # TODO: deinfe amx feat, feat on train and test, tranfsofrm on individuals\n",
        "    for i in range(14):#range(data.shape[0]//10):\n",
        "        train_data = vectorize.transform(data['X'][i:i+1000])\n",
        "\n",
        "        clf.fit(train_data, data['label'][i:i+1000])\n",
        "    X_test = vectorize.transform(data['X'][15000:15555])\n",
        "    y_test = data['label'][15000:15555]\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # vectorizer = to_tdidf()\n",
        "    # with open(os.path.join(MY_HOME,\"tdidf_list.pkl\"), 'rb') as jar:\n",
        "    #     dense_arr = pickle.load(jar)\n",
        "\n",
        "    # print(dense_arr)\n",
        "    # data = pd.read_pickle(os.path.join(MY_HOME, \"preprocessed_training_dataframe.pkl\"))\n",
        "    # print(data.shape)\n",
        "    # fc(vectorizer, dense_arr, data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========> Data Preprocessing\n",
            "    label                                                  X\n",
            "id                                                          \n",
            "0       0   a lin from georg orwel 's novel 1984 predict ...\n",
            "1       2   main legisl candid les gibson insult parkland...\n",
            "4       1   a 17-year-old girl nam alyss carson be be tra...\n",
            "5       2   in 1988 auth roald dahl pen an op let urg par...\n",
            "6       2  hil clinton when it com to fight ter , `` anot...\n",
            "(15555, 2)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.70      0.64       261\n",
            "           1       0.52      0.50      0.51       231\n",
            "           2       0.09      0.03      0.05        63\n",
            "\n",
            "    accuracy                           0.54       555\n",
            "   macro avg       0.40      0.41      0.40       555\n",
            "weighted avg       0.50      0.54      0.52       555\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6vcbT3cK0g8",
        "colab_type": "code",
        "outputId": "d5630288-14ff-4e63-90e7-d4f229eb03e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.7)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.4.2)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMGLSlF6R9YW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmjGYZrsSMnz",
        "colab_type": "code",
        "outputId": "0d359a53-6715-44f5-9e34-a7b7e300437b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "# Import the corpus data\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTiNSeIdSV2X",
        "colab_type": "code",
        "outputId": "18c98979-d6cb-40f7-8c67-a5b9c895e543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!ls drive/My\\ Drive\n",
        "MY_HOME = 'drive/My\\ Drive/' #metadata_articles_dataframe.pkl\n",
        "!ls drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3tdQdMLcFuG",
        "colab_type": "text"
      },
      "source": [
        "# KERAS to try the word embedding method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOh6gDWEcEN9",
        "colab_type": "code",
        "outputId": "bad6bfc5-2bde-4ae0-c04e-4d62330f0115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "\n",
        "data = pd.read_pickle(os.path.join(MY_HOME, \"nlp/preprocessed_training_dataframe.pkl\"))\n",
        "# define documents\n",
        "docs = data['X'][0:12000]\n",
        "# define class labels\n",
        "labels = data['label'][0:12000]\n",
        "print(labels)\n",
        "# convert labels to onehot\n",
        "labels = one_hot(labels, 3)\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 300\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open(os.path.join(MY_HOME, 'nlp/glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 50))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=2, verbose=0)\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id\n",
            "0        0\n",
            "1        2\n",
            "4        1\n",
            "5        2\n",
            "6        2\n",
            "        ..\n",
            "13211    2\n",
            "13212    0\n",
            "13213    1\n",
            "13214    1\n",
            "13215    0\n",
            "Name: label, Length: 12000, dtype: int64\n",
            "Loaded 400000 word vectors.\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 300, 50)           16058550  \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 15000)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 45003     \n",
            "=================================================================\n",
            "Total params: 16,103,553\n",
            "Trainable params: 45,003\n",
            "Non-trainable params: 16,058,550\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5adf15de10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipOjvbfMcCyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7782bfbe-7a5c-4024-c4c3-8ae7fa032521"
      },
      "source": [
        "# evaluate the model\n",
        "# define documents\n",
        "docs = data['X'][12000:15555]\n",
        "# define class labels\n",
        "labels = data['label'][12000:15555]\n",
        "print(labels)\n",
        "# convert labels to onehot\n",
        "labels = one_hot(labels, 3)\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 300\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id\n",
            "13216    1\n",
            "13217    0\n",
            "13218    1\n",
            "13219    2\n",
            "13220    0\n",
            "        ..\n",
            "17137    2\n",
            "17138    0\n",
            "17139    0\n",
            "17140    2\n",
            "17141    1\n",
            "Name: label, Length: 3555, dtype: int64\n",
            "Accuracy: 62.512894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n49pRSfNKtbA",
        "colab_type": "code",
        "outputId": "89e08372-79c7-48c7-e96a-f065e8da5274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "!pip install torch<=1.2.0\n",
        "!pip install torchtext==0.4\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: =1.2.0: No such file or directory\n",
            "Collecting torchtext==0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.3.1+cu100)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}