{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from preprocessing import preprocess, create_dataframe_for_training\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout, Concatenate, concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "PREPROCESSING = False\n",
    "REMOVE_STOP_WORDS = False\n",
    "THRESHOLD_INFREQUENT_WORDS = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "PREPROC_FILEPATH = os.path.join(DATA_DIR, 'preprocessed_training_dataframe.pkl')\n",
    "DATA_FILEPATH = os.path.join(DATA_DIR, 'metadata_articles_dataframe.pkl')\n",
    "\n",
    "data = pd.read_pickle(DATA_FILEPATH)\n",
    "#preproc = pd.read_pickle(PREPROC_FILEPATH)\n",
    "#data = data[:1000]\n",
    "\n",
    "def generate_feature_matrix(X):\n",
    "    \n",
    "    #X = create_dataframe_for_training(X)\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=0.01)\n",
    "    X_article_fe = vectorizer.fit_transform(X[\"article_content\"])\n",
    "    X_claim_fe = vectorizer.transform(X[\"claim\"])\n",
    "    return X_claim_fe.toarray(), X_article_fe.toarray()\n",
    "\n",
    "\n",
    "# X_claim_fe, X_article_fe = generate_feature_matrix(data)\n",
    "\n",
    "# print(\"X_claim_fe matrix shape: \" + str(X_claim_fe.shape))\n",
    "# print(\"X_article_fe matrix shape: \" + str(X_article_fe.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "df['article_content'] = df['article_content'].apply(clean_text)\n",
    "df['article_content'] = df['article_content'].str.replace('\\d+', '')\n",
    "\n",
    "df['claim'] = df['claim'].apply(clean_text)\n",
    "df['claim'] = df['claim'].str.replace('\\d+', '')\n",
    "\n",
    "df.loc[df['claimant'] == \"\", \"claimant\"] = \"unknown\"\n",
    "df[\"num_related_articles\"] = df[\"related_articles\"].apply(lambda x: len(x))\n",
    "df['num_date'] = pd.to_numeric(df['date'].dt.strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1131145 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max N words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['article_content'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['article_content'].values)\n",
    "X_article = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X = tokenizer.texts_to_sequences(df['claim'].values)\n",
    "X_claim = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# X_claim = np.array(X_claim_fe)\n",
    "# X_article = np.array(X_article_fe)\n",
    "\n",
    "X_num_articles = df[\"num_related_articles\"].to_numpy().reshape(-1,1)\n",
    "X_claimant = pd.get_dummies(df['claimant']).values\n",
    "X_date = df[\"num_date\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "X_numeric = np.concatenate((X_num_articles, X_claimant, X_date), axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_numeric = scaler.fit_transform(X_numeric)\n",
    "\n",
    "#X_final = np.concatenate((X_article, X_claim, X_num_articles, X_claimant, X_date), axis=1)\n",
    "#print('Shape of data tensor:', X_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (15555, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['label']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_idx = np.where(np.logical_or(data[\"fold\"] == \"train\", data[\"fold\"] == \"development\"))\n",
    "#dev_idx = np.where(data[\"fold\"] == \"development\")\n",
    "test_idx = np.where(data[\"fold\"] == \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sets of inputs\n",
    "numeric_input = Input(shape=(X_numeric.shape[1],))\n",
    "claim = Input(shape=(X_claim.shape[1],))\n",
    "article = Input(shape=(X_article.shape[1],))\n",
    "  \n",
    "y = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_claim.shape[1])(claim)\n",
    "z = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_article.shape[1])(article)\n",
    "\n",
    "combined_bilstm = concatenate([y, z])\n",
    "combined_bilstm = SpatialDropout1D(0.4)(combined_bilstm)\n",
    "combined_bilstm = Bidirectional(LSTM(64))(combined_bilstm)\n",
    "combined_bilstm = Dropout(0.5)(combined_bilstm)\n",
    "combined_bilstm = Dense(9, activation='relu')(combined_bilstm)\n",
    "\n",
    "numeric_feat = Dense(64, activation=\"relu\")(numeric_input)\n",
    "numeric_feat = Dense(3, activation=\"relu\")(numeric_feat)\n",
    "\n",
    "final_model = concatenate([numeric_feat, combined_bilstm])\n",
    "final_model = Dense(10, activation='relu')(final_model)\n",
    "final_model = Dense(3, activation='softmax')(final_model)\n",
    "\n",
    "model = Model(inputs=[numeric_input, claim, article], outputs=final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balance target for better F1 score prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx = np.where(np.logical_and(df['label']==0, np.logical_or(data[\"fold\"] == \"train\", data[\"fold\"] == \"development\")))\n",
    "partly_true_idx = np.where(np.logical_and(df['label']==1, np.logical_or(data[\"fold\"] == \"train\", data[\"fold\"] == \"development\")))\n",
    "false_idx = np.where(np.logical_and(df['label']==2, np.logical_or(data[\"fold\"] == \"train\", data[\"fold\"] == \"development\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebal_modeling_idx = np.concatenate((true_idx[0][:len(false_idx[0])], partly_true_idx[0][:len(false_idx[0])], false_idx[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on original dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13222 samples, validate on 2333 samples\n",
      "Epoch 1/2\n",
      "13222/13222 [==============================] - 809s 61ms/step - loss: 0.9753 - acc: 0.5209 - val_loss: 0.9318 - val_acc: 0.5641\n",
      "Epoch 2/2\n",
      "13222/13222 [==============================] - 801s 61ms/step - loss: 0.8778 - acc: 0.6124 - val_loss: 0.9262 - val_acc: 0.5757\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch_size = 256\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "#adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"Nadam\", metrics=['accuracy'])\n",
    "\n",
    "print('Train on original dataset')\n",
    "history = model.fit([X_numeric[modeling_idx], X_claim[modeling_idx], X_article[modeling_idx]], Y[modeling_idx], epochs=epochs, batch_size=batch_size,validation_data=([X_numeric[test_idx], X_claim[test_idx], X_article[test_idx]], Y[test_idx]) ,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "#print('Train on rebalanced dataset')\n",
    "#history2 = model.fit([X_numeric[rebal_modeling_idx], X_claim[rebal_modeling_idx], X_article[rebal_modeling_idx]], Y[rebal_modeling_idx], epochs=epochs, batch_size=batch_size,validation_data=([X_numeric[test_idx], X_claim[test_idx], X_article[test_idx]], Y[test_idx]) ,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on rebalanced dataset\n",
      "Train on 4347 samples, validate on 2333 samples\n",
      "Epoch 1/2\n",
      "4347/4347 [==============================] - 298s 69ms/step - loss: 0.9933 - acc: 0.4661 - val_loss: 1.0071 - val_acc: 0.5817\n",
      "Epoch 2/2\n",
      "4347/4347 [==============================] - 296s 68ms/step - loss: 0.9748 - acc: 0.4799 - val_loss: 1.0482 - val_acc: 0.5671\n"
     ]
    }
   ],
   "source": [
    "train_rebalanced = True\n",
    "if train_rebalanced:   \n",
    "    epochs = 2\n",
    "    batch_size = 256\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    #adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"Nadam\", metrics=['accuracy'])\n",
    "\n",
    "    print('Train on rebalanced dataset')\n",
    "    history2 = model.fit([X_numeric[rebal_modeling_idx], X_claim[rebal_modeling_idx], X_article[rebal_modeling_idx]], Y[rebal_modeling_idx], epochs=epochs, batch_size=batch_size,validation_data=([X_numeric[test_idx], X_claim[test_idx], X_article[test_idx]], Y[test_idx]) ,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([X_numeric[test_idx], X_claim[test_idx], X_article[test_idx]])\n",
    "\n",
    "pred_class = np.argmax(pred, axis=1)\n",
    "pd.DataFrame(pred_class).to_pickle(os.path.join(PROJECT_DIR, \"predictions\\\\predictions_bilstm_final_rebal.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1683\n",
       "1     574\n",
       "2      76\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred_class)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
