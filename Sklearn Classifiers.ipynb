{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Classifiers\n",
    "Classifiers trained on only the claimant (metadata) data. No article content is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "# from datacup_utils import score1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 15555 observations.\n",
      "There is a total of 6 features.\n",
      "There is a total of 3 classes.\n",
      "Classes are: [0 2 1]\n",
      "Here's the count of labels:\n",
      "    FALSE (0): 7408\n",
      "    PARTIALLY_TRUE (1): 6451\n",
      "    TRUE (2): 1696\n"
     ]
    }
   ],
   "source": [
    "labels_dict = {0:\"FALSE\", 1:\"PARTIALLY_TRUE\", 2:\"TRUE\"}\n",
    "def get_data_from_json():\n",
    "    path_to_data = \"data/train.json\"\n",
    "    data = pd.read_json(path_to_data)\n",
    "    print(\"There is a total of %s observations.\" % (len(data)))\n",
    "    print(\"There is a total of %s features.\" % (len(data.columns)))\n",
    "    print(\"There is a total of %s classes.\" % (len(data.label.unique())))\n",
    "    print(\"Classes are: %s\" % ((data.label.unique())))\n",
    "    print(\"Here's the count of labels:\")\n",
    "    vc = data.label.value_counts()\n",
    "    [print(\"    \" + labels_dict[label] + \" (\" + str(label) + \"): \" + str(count)) for count,label \\\n",
    "                     in zip(vc.values,vc.index)]\n",
    "    return data\n",
    "data = get_data_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"date\",\"claim\",\"id\",\"claimant\",\"related_articles\"]],\n",
    "                                                    data[['label']], test_size=0.3, random_state=24)\n",
    "\n",
    "X_train.to_json(\"X_train.json\")\n",
    "X_test.to_json(\"X_test.json\")\n",
    "y_train.to_json(\"y_train.json\")\n",
    "y_test.to_json(\"y_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features\n",
    "#### Using only the training set --> X_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_json(\"X_train.json\")\n",
    "y = pd.read_json(\"y_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 3481\n",
       "Donald Trump                                      895\n",
       "Bloggers                                          258\n",
       "Barack Obama                                      163\n",
       "Hillary Clinton                                   155\n",
       "Viral image                                        98\n",
       "Bernie Sanders                                     74\n",
       "Ted Cruz                                           74\n",
       "Facebook posts                                     73\n",
       "Various websites                                   73\n",
       "Marco Rubio                                        67\n",
       "Scott Walker                                       62\n",
       "John McCain                                        58\n",
       "Rick Perry                                         57\n",
       "Rick Scott                                         51\n",
       "Facebook user                                      48\n",
       "multiple sources                                   46\n",
       "Chain email                                        44\n",
       "Mike Pence                                         44\n",
       "Multiple sources                                   42\n",
       "Jeb Bush                                           41\n",
       "Charlie Crist                                      31\n",
       "Vladimir Putin                                     30\n",
       "Ron Johnson                                        30\n",
       "Paul Ryan                                          30\n",
       "Greg Abbott                                        26\n",
       "Tim Kaine                                          26\n",
       "Tammy Baldwin                                      26\n",
       "Chris Christie                                     26\n",
       "Joe Biden                                          26\n",
       "                                                 ... \n",
       "Jean  Casarez                                       1\n",
       "Dmitry Polyanskiy                                   1\n",
       "Ministry of Internal Affairs (Police), Russia       1\n",
       "Andy Puzder                                         1\n",
       "Samuel A. Alito                                     1\n",
       "Brian Schweitzer                                    1\n",
       "Oregon Republican Party                             1\n",
       "Frank Annunziato                                    1\n",
       "Susan Pitman                                        1\n",
       "Douglas Holtz-Eakin                                 1\n",
       "Sonny Perdue                                        1\n",
       "Fela Durotoye                                       1\n",
       "Boris Johnson                                       1\n",
       "Richard  Ferruccio                                  1\n",
       "Nikki Bowmar                                        1\n",
       "Jeanine  Pirro                                      1\n",
       "Rodney Ellis                                        1\n",
       "Liberty Headlines                                   1\n",
       "Ed Emery                                            1\n",
       "Semyon Bagdasarov                                   1\n",
       "Bill de Blasio                                      1\n",
       "BlueNC                                              1\n",
       "Clint Eastwood                                      1\n",
       "Jeff Clemens                                        1\n",
       "Mark Pryor                                          1\n",
       "William Barr                                        1\n",
       "Upendra Chivukula                                   1\n",
       "James Clyburn                                       1\n",
       "The Coalition for Public Schools                    1\n",
       "Tina Rosenberg                                      1\n",
       "Name: claimant, Length: 2440, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.claimant.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new features\n",
    "# number of related articles\n",
    "X[\"num_related_articles\"] = X[\"related_articles\"].apply(lambda x: len(x))\n",
    "# claimant as one-hot-encoding\n",
    "X['date'] = pd.to_numeric(X['date'])\n",
    "\n",
    "s = X['claimant'].value_counts()\n",
    "X['claimant'] = np.where(X['claimant'].isin(s.index[s <= 3]), 'Other', X['claimant'])\n",
    "\n",
    "dummies = pd.get_dummies(X.claimant, prefix=\"claimant\", prefix_sep=\"_\")\n",
    "X = pd.concat([X[[\"date\",\"id\", \"num_related_articles\"]],dummies], axis=1).sort_index(axis=1)\n",
    "\n",
    "col_in_train = np.append(dummies.columns.values, [[\"date\",\"id\", \"num_related_articles\"]])\n",
    "claimant_columns = dummies.columns.values\n",
    "\n",
    "with open(\"claiment_list.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(claimant_columns, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "MAX: 41.91%\n",
      "MIN: 40.93%\n",
      "AVG: 41.46%\n",
      "\n",
      "Random Forest\n",
      "MAX: 41.81%\n",
      "MIN: 40.49%\n",
      "AVG: 41.19%\n",
      "\n",
      "AdaBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX: 41.92%\n",
      "MIN: 40.02%\n",
      "AVG: 40.75%\n",
      "\n",
      "Naive Babes\n",
      "MAX: 35.61%\n",
      "MIN: 35.24%\n",
      "AVG: 35.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(59,n_jobs=-1),\n",
    "    DecisionTreeClassifier(max_depth=8),\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=50,n_jobs=-1),\n",
    "    AdaBoostClassifier(n_estimators=96, learning_rate=1.5),\n",
    "    GaussianNB(),\n",
    "\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "#     SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "\n",
    "names = [\n",
    "#          \"Nearest Neighbors\",\n",
    "         \"Decision Tree\", \n",
    "         \"Random Forest\", \n",
    "         \"AdaBoost\",\n",
    "         \"Naive Babes\",\n",
    "        \n",
    "#          \"Linear SVM\", \n",
    "#          \"RBF SVM\",\n",
    "#          \"Gaussian Process\",\n",
    "#          \"Neural Net\",\n",
    "#          \"QDA\"\n",
    "        ]\n",
    "\n",
    "weights = []\n",
    "for clf,name in zip(classifiers,names):\n",
    "    print(name)\n",
    "    trained_models = cross_validate(clf, X, y.values.ravel(),\n",
    "                                    cv=3, scoring=make_scorer(score1, greater_is_better=True),\n",
    "                                    return_estimator=1)\n",
    "    results = trained_models['test_score']\n",
    "\n",
    "    print(\"MAX: %s\\nMIN: %s\\nAVG: %s\\n\" % ('{:.2%}'.format(max(results)),\n",
    "                                           '{:.2%}'.format(min(results)),\n",
    "                                           '{:.2%}'.format(np.mean(results))))\n",
    "    weights.append(np.mean(results))\n",
    "    best_trained_model = trained_models['estimator'][\\\n",
    "                        list(trained_models['test_score']).index(max(trained_models['test_score']))]\n",
    "\n",
    "    pickle.dump(best_trained_model, open(\"models/\" + name + \".sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
